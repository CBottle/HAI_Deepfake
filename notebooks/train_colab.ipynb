{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI Deepfake Detection - Colab Training Pipeline\n",
    "\n",
    "**4ê°€ì§€ í•µì‹¬ ì„ë¬´:**\n",
    "1. **ì½”ë“œ ë™ê¸°í™” (Git Sync)** - ë¡œì»¬ì—ì„œ pushí•œ ì½”ë“œë¥¼ Colabì— ìë™ ë™ê¸°í™”\n",
    "2. **ìë™ í™˜ê²½ ì„¸íŒ… (Environment Setup)** - í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ìë™ ì„¤ì¹˜\n",
    "3. **ë°ì´í„° ê³ ì†ë„ë¡œ (Data Loading)** - Kaggle APIë¡œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì§ì ‘ ë‹¤ìš´ë¡œë“œ\n",
    "4. **ê¸ˆê³  ë³´ê´€ (Model Persistence)** - Google Driveì— í•™ìŠµëœ ëª¨ë¸ ìë™ ë°±ì—…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. GPU í™•ì¸\n",
    "Colabì—ì„œ GPUê°€ í™œì„±í™”ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA ë²„ì „: {torch.version.cuda}\")\n",
    "else:\n",
    "    print(\"GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€\" \\\n",
    "    \"ê²½ > GPU ì„ íƒ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. ê¸ˆê³  ë³´ê´€ (Model Persistence) - Google Drive ë§ˆìš´íŠ¸\n",
    "í•™ìŠµëœ ëª¨ë¸ì„ ì•ˆì „í•˜ê²Œ ë³´ê´€í•˜ê¸° ìœ„í•´ Google Driveë¥¼ ì—°ê²°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "import os\n",
    "DRIVE_BACKUP_DIR = '/content/drive/MyDrive/HAI_Deepfake'\n",
    "os.makedirs(f\"{DRIVE_BACKUP_DIR}/models\", exist_ok=True)\n",
    "os.makedirs(f\"{DRIVE_BACKUP_DIR}/checkpoints\", exist_ok=True)\n",
    "print(f\"ë°±ì—… ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì™„ë£Œ: {DRIVE_BACKUP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ì½”ë“œ ë™ê¸°í™” (Git Sync)\n",
    "GitHubì—ì„œ ìµœì‹  ì½”ë“œë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤. ë¡œì»¬ì—ì„œ `git push` í›„ ì´ ì…€ë§Œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# GitHub ì„¤ì •\n",
    "REPO_URL = \"https://github.com/CBottle/HAI_Deepfake.git\"\n",
    "PROJECT_DIR = \"/content/HAI_Deepfake\"\n",
    "BRANCH = \"HK\"  # <--- ë™ê¸°í™”í•  ë¸Œëœì¹˜ ì´ë¦„ (HK ë˜ëŠ” main)\n",
    "\n",
    "if os.path.exists(PROJECT_DIR):\n",
    "    print(f\"ê¸°ì¡´ í”„ë¡œì íŠ¸ ë°œê²¬! {BRANCH} ë¸Œëœì¹˜ ìµœì‹  ì½”ë“œë¡œ ì—…ë°ì´íŠ¸ ì¤‘...\")\n",
    "    %cd {PROJECT_DIR}\n",
    "    !git fetch --all\n",
    "    # ë¸Œëœì¹˜ ì „í™˜ ë° ë¦¬ì…‹\n",
    "    !git checkout {BRANCH} || git checkout -b {BRANCH} origin/{BRANCH}\n",
    "    !git reset --hard origin/{BRANCH}\n",
    "    print(f\"\\n{BRANCH} ë¸Œëœì¹˜ ì½”ë“œ ë™ê¸°í™” ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(f\"í”„ë¡œì íŠ¸ í´ë¡  ì¤‘... (ë¸Œëœì¹˜: {BRANCH})\")\n",
    "    !git clone -b {BRANCH} {REPO_URL} {PROJECT_DIR}\n",
    "    %cd {PROJECT_DIR}\n",
    "    print(f\"\\ní´ë¡  ë° {BRANCH} ë¸Œëœì¹˜ ì „í™˜ ì™„ë£Œ!\")\n",
    "\n",
    "# í˜„ì¬ ì»¤ë°‹ ë° ë¸Œëœì¹˜ í™•ì¸\n",
    "print(\"\\ní˜„ì¬ ì •ë³´:\")\n",
    "!git branch --show-current\n",
    "!git log -1 --oneline\n",
    "print(f\"\\nì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ìë™ í™˜ê²½ ì„¸íŒ… (Environment Setup)\n",
    "`requirements.txt`ë¥¼ ì½ì–´ í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# requirements.txt ê²½ë¡œ í™•ì¸\n",
    "req_paths = ['requirements.txt', 'env/requirements.txt']\n",
    "req_file = None\n",
    "\n",
    "for path in req_paths:\n",
    "    if os.path.exists(path):\n",
    "        req_file = path\n",
    "        break\n",
    "\n",
    "if req_file:\n",
    "    print(f\"íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘: {req_file}\")\n",
    "    print(\"=\"*50)\n",
    "    !pip install -q -r {req_file}\n",
    "    print(\"=\"*50)\n",
    "    print(\"í™˜ê²½ ì„¤ì • ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(\"requirements.txtë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ê¸°ë³¸ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤...\")\n",
    "    !pip install -q torch torchvision transformers pillow opencv-python scikit-learn tqdm pyyaml\n",
    "    !pip install mediapipe qdm opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. ë°ì´í„° ê³ ì†ë„ë¡œ (Data Loading) - Kaggle API\n",
    "Kaggle ì„œë²„ì—ì„œ Colab ì„œë²„ë¡œ ë°ì´í„°ë¥¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤. (ë‚´ ì»´í“¨í„°ë¥¼ ê±°ì¹˜ì§€ ì•Šì•„ ë¹ ë¦„!)\n",
    "\n",
    "**ì‚¬ì „ ì¤€ë¹„:**\n",
    "1. Kaggle ê³„ì •ì—ì„œ `kaggle.json` ë‹¤ìš´ë¡œë“œ (Account > API > Create New Token)\n",
    "2. Google Driveì˜ `/MyDrive/Kaggle/kaggle.json`ì— ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import os\n",
    "\n",
    "# Kaggle API ì„¤ì •\n",
    "KAGGLE_CONFIG_DIR = '/root/.kaggle'\n",
    "DRIVE_KAGGLE_KEY = '/content/drive/MyDrive/Kaggle/kaggle.json'\n",
    "\n",
    "os.makedirs(KAGGLE_CONFIG_DIR, exist_ok=True)\n",
    "\n",
    "if os.path.exists(DRIVE_KAGGLE_KEY):\n",
    "    !cp {DRIVE_KAGGLE_KEY} {KAGGLE_CONFIG_DIR}/\n",
    "    !chmod 600 {KAGGLE_CONFIG_DIR}/kaggle.json\n",
    "    print(\"Kaggle API í‚¤ ì„¤ì • ì™„ë£Œ!\")\n",
    "    KAGGLE_READY = True\n",
    "else:\n",
    "    print(\"kaggle.jsonì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(f\"Google Driveì— íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”: {DRIVE_KAGGLE_KEY}\")\n",
    "    KAGGLE_READY = False\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ============================================\n",
    "# Kaggle ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
    "# ì•„ë˜ DATASET_NAMEì„ ë³¸ì¸ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”!\n",
    "# ============================================\n",
    "\n",
    "# ì˜ˆì‹œ ë°ì´í„°ì…‹ (ë³€ê²½ í•„ìš”)\n",
    "DATASET_NAME = \"manjilkarki/deepfake-and-real-images\"  # Kaggle ë°ì´í„°ì…‹ ê²½ë¡œ\n",
    "DATA_DIR = \"./train_data\"\n",
    "\n",
    "if KAGGLE_READY:\n",
    "    # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸\n",
    "    if os.path.exists(DATA_DIR) and len(os.listdir(DATA_DIR)) > 0:\n",
    "        print(f\"ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {DATA_DIR}\")\n",
    "        !ls -la {DATA_DIR}\n",
    "    else:\n",
    "        print(f\"ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘: {DATASET_NAME}\")\n",
    "        !kaggle datasets download -d {DATASET_NAME} -p {DATA_DIR} --unzip\n",
    "        print(\"ë‹¤ìš´ë¡œë“œ ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(\"Kaggle APIê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...\")\n",
    "    !python create_dummy_data.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ìš´ë¡œë“œX êµ¬ê¸€ ë“œë¼ì´ë¸Œì—ì„œ zipí´ë” ê°€ì§€ê³  ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/HAI_Deepfake/extracted_frames.zip /content/extracted_data.zip\n",
    "!unzip -q /content/extracted_data.zip -d /content/train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. í•™ìŠµ ì‹¤í–‰\n",
    "GPUë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"ğŸ“Š í•™ìŠµ ë°ì´í„° êµ¬ì¡° ë° ê°œìˆ˜ í™•ì¸\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ê¸°ì¤€ì´ ë˜ëŠ” ìµœìƒìœ„ ê²½ë¡œ\n",
    "base_path = '/content/train_data'\n",
    "\n",
    "if os.path.exists(base_path):\n",
    "    # í´ë” êµ¬ì¡° ë¦¬ìŠ¤íŠ¸ ì¶œë ¥ (í™•ì¸ìš©)\n",
    "    print(\"ğŸ“ í˜„ì¬ í´ë” ëª©ë¡:\")\n",
    "    !ls -F {base_path}\n",
    "    \n",
    "    # --- Train ë°ì´í„° í™•ì¸ ---\n",
    "    train_path = os.path.join(base_path, 'Train')\n",
    "    if os.path.exists(train_path):\n",
    "        # ê²½ë¡œ ë’¤ì˜ Real, Fake ëŒ€ì†Œë¬¸ìê°€ ì‹¤ì œ í´ë”ëª…ê³¼ ê°™ì€ì§€ ê¼­ í™•ì¸í•´!\n",
    "        t_real_path = os.path.join(train_path, 'Real')\n",
    "        t_fake_path = os.path.join(train_path, 'Fake')\n",
    "        \n",
    "        real_count = len(os.listdir(t_real_path)) if os.path.exists(t_real_path) else 0\n",
    "        fake_count = len(os.listdir(t_fake_path)) if os.path.exists(t_fake_path) else 0\n",
    "        \n",
    "        print(f\"\\n[Train Set]\")\n",
    "        print(f\"  - Real ì´ë¯¸ì§€: {real_count:,}ê°œ\")\n",
    "        print(f\"  - Fake ì´ë¯¸ì§€: {fake_count:,}ê°œ\")\n",
    "    \n",
    "    # --- Validation ë°ì´í„° í™•ì¸ ---\n",
    "    val_path = os.path.join(base_path, 'Validation')\n",
    "    if os.path.exists(val_path):\n",
    "        v_real_path = os.path.join(val_path, 'Real')\n",
    "        v_fake_path = os.path.join(val_path, 'Fake')\n",
    "        \n",
    "        real_count = len(os.listdir(v_real_path)) if os.path.exists(v_real_path) else 0\n",
    "        fake_count = len(os.listdir(v_fake_path)) if os.path.exists(v_fake_path) else 0\n",
    "        \n",
    "        print(f\"\\n[Validation Set]\")\n",
    "        print(f\"  - Real ì´ë¯¸ì§€: {real_count:,}ê°œ\")\n",
    "        print(f\"  - Fake ì´ë¯¸ì§€: {fake_count:,}ê°œ\")\n",
    "\n",
    "else:\n",
    "    print(f\"âŒ '{base_path}' í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤! ê²½ë¡œë¥¼ ë‹¤ì‹œ í™•ì¸í•´ì£¼ì„¸ìš”.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# 1. train_loaderì—ì„œ ë°°ì¹˜ í•˜ë‚˜ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "# train_loaderê°€ ì •ì˜ëœ ì´í›„ì— ì‹¤í–‰í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "data_iter = iter(train_loader)\n",
    "batch = next(data_iter)\n",
    "\n",
    "# 2. ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "# ViTImageProcessorë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ 'pixel_values' í‚¤ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "images = batch['pixel_values']\n",
    "labels = batch['labels']\n",
    "\n",
    "# 3. ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ê³  ì°¨ì›ì„ ë³€ê²½í•©ë‹ˆë‹¤.\n",
    "# PyTorch: (Batch, Channel, Height, Width) -> Matplotlib: (Height, Width, Channel)\n",
    "img = images[0].permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# 4. ì—­ì •ê·œí™”(De-normalization)\n",
    "# ViT í”„ë¡œì„¸ì„œëŠ” ë³´í†µ ì´ë¯¸ì§€ë¥¼ 0.5 í‰ê· , 0.5 í‘œì¤€í¸ì°¨ë¡œ ì •ê·œí™”í•©ë‹ˆë‹¤.\n",
    "# ì´ë¥¼ ë‹¤ì‹œ 0~1 ì‚¬ì´ë¡œ ë³µêµ¬í•´ì•¼ ëˆˆì— ë³´ì´ëŠ” ì´ë¯¸ì§€ê°€ ë©ë‹ˆë‹¤.\n",
    "img = img * 0.5 + 0.5\n",
    "img = np.clip(img, 0, 1)\n",
    "\n",
    "# 5. ì‹œê°í™”\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(img)\n",
    "plt.title(f\"Label: {'Fake' if labels[0] == 1 else 'Real'}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì‹¤í–‰\n",
    "!python train.py --config config/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. (ì„ íƒ) ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¶”ë¡ ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/HAI_Deepfake/open.zip /content/test_data.zip\n",
    "!unzip -q /content/test_data.zip -d /content/test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¶”ë¡  ì‹¤í–‰ (ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ í™•ì¸ í•„ìš”)\n",
    "!python inference.py \\\n",
    "    --model /content/drive/MyDrive/HAI_Deepfake/checkpoints/best_model.pt \\\n",
    "    --test_dir /content/test_data/test_data \\\n",
    "    --output submissions/submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ë¹ ë¥¸ ì‹¤í–‰ ê°€ì´ë“œ\n",
    "\n",
    "1. **ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > GPU ì„ íƒ**\n",
    "2. **ëŸ°íƒ€ì„ > ëª¨ë‘ ì‹¤í–‰** ë˜ëŠ” `Ctrl+F9`\n",
    "3. Google Drive ë§ˆìš´íŠ¸ ìŠ¹ì¸\n",
    "4. í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ì€ ìë™ìœ¼ë¡œ Driveì— ë°±ì—…ë¨\n",
    "\n",
    "### ì½”ë“œ ì—…ë°ì´íŠ¸ ì‹œ\n",
    "ë¡œì»¬ì—ì„œ `git push` í›„, **ì„¹ì…˜ 2 (ì½”ë“œ ë™ê¸°í™”)** ì…€ë§Œ ë‹¤ì‹œ ì‹¤í–‰í•˜ë©´ ë©ë‹ˆë‹¤!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
