{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ HAI Deepfake - ë°ì´í„° ì¤€ë¹„ ë…¸íŠ¸ë¶ (Colab)\n",
    "\n",
    "**ì´ ë…¸íŠ¸ë¶ì˜ ëª©ì :**\n",
    "1. âœ… Kaggle ë°ì´í„°ì…‹ì„ Google Driveë¡œ ë‹¤ìš´ë¡œë“œ\n",
    "2. âœ… ë¹„ë””ì˜¤ â†’ ì´ë¯¸ì§€ í”„ë ˆì„ ì¶”ì¶œ\n",
    "3. âœ… ì†Œê·œëª¨ ë°ì´í„°ì…‹ ìƒì„± (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)\n",
    "4. âœ… ë°ì´í„° ê²€ì¦\n",
    "\n",
    "**ì‘ì—… ìˆœì„œ:**\n",
    "- ë‹¨ê³„ë³„ë¡œ ì…€ì„ ì‹¤í–‰í•˜ì„¸ìš”\n",
    "- ì²« ì‹¤í–‰ ì‹œ ì „ì²´ ì‹¤í–‰ (ì•½ 30ë¶„~1ì‹œê°„)\n",
    "- ì´í›„ì—ëŠ” í•„ìš”í•œ ì„¹ì…˜ë§Œ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Œ Step 0: í™˜ê²½ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ğŸ” í™˜ê²½ ì •ë³´\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Python: {sys.version.split()[0]}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   CUDA: {torch.version.cuda}\")\n",
    "    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"âš ï¸  GPU ì—†ìŒ - 'ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½ > GPU' ì„ íƒí•˜ì„¸ìš”!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Œ Step 1: Google Drive ë§ˆìš´íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Google Drive ë§ˆìš´íŠ¸\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì •\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive/HAI_Deepfake\")\n",
    "DRIVE_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "# í•˜ìœ„ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "(DRIVE_ROOT / \"datasets\").mkdir(exist_ok=True)\n",
    "(DRIVE_ROOT / \"train_data\").mkdir(exist_ok=True)\n",
    "(DRIVE_ROOT / \"train_data_small\").mkdir(exist_ok=True)\n",
    "(DRIVE_ROOT / \"models\").mkdir(exist_ok=True)\n",
    "(DRIVE_ROOT / \"checkpoints\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"âœ… Google Drive ë§ˆìš´íŠ¸ ì™„ë£Œ!\")\n",
    "print(f\"ğŸ“ í”„ë¡œì íŠ¸ ë£¨íŠ¸: {DRIVE_ROOT}\")\n",
    "print(f\"\\nğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡°:\")\n",
    "print(f\"  {DRIVE_ROOT}/\")\n",
    "print(f\"  â”œâ”€â”€ datasets/         # Kaggle ì›ë³¸ ë°ì´í„°\")\n",
    "print(f\"  â”œâ”€â”€ train_data/       # ì „ì²˜ë¦¬ëœ í•™ìŠµ ë°ì´í„°\")\n",
    "print(f\"  â”œâ”€â”€ train_data_small/ # ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ ë°ì´í„°\")\n",
    "print(f\"  â”œâ”€â”€ models/           # ìµœì¢… ëª¨ë¸\")\n",
    "print(f\"  â””â”€â”€ checkpoints/      # í•™ìŠµ ì²´í¬í¬ì¸íŠ¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Œ Step 2: í”„ë¡œì íŠ¸ ì½”ë“œ ë™ê¸°í™” (Git)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_URL = \"https://github.com/CBottle/HAI_Deepfake.git\"\n",
    "PROJECT_DIR = \"/content/HAI_Deepfake\"\n",
    "\n",
    "if os.path.exists(PROJECT_DIR):\n",
    "    print(\"ğŸ”„ ê¸°ì¡´ ì½”ë“œ ì—…ë°ì´íŠ¸ ì¤‘...\")\n",
    "    %cd {PROJECT_DIR}\n",
    "    !git fetch --all\n",
    "    !git reset --hard origin/main\n",
    "    print(\"âœ… ì½”ë“œ ì—…ë°ì´íŠ¸ ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(\"ğŸ“¥ í”„ë¡œì íŠ¸ í´ë¡  ì¤‘...\")\n",
    "    !git clone {REPO_URL} {PROJECT_DIR}\n",
    "    %cd {PROJECT_DIR}\n",
    "    print(\"âœ… í´ë¡  ì™„ë£Œ!\")\n",
    "\n",
    "# ì‘ì—… ë””ë ‰í† ë¦¬ í™•ì¸\n",
    "print(f\"\\nğŸ“ í˜„ì¬ ìœ„ì¹˜: {os.getcwd()}\")\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Œ Step 3: í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“¦ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ì¤‘...\\n\")\n",
    "\n",
    "# requirements.txtê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì§ì ‘ ì„¤ì¹˜\n",
    "if os.path.exists(\"env/requirements.txt\"):\n",
    "    !pip install -q -r env/requirements.txt\n",
    "else:\n",
    "    !pip install -q kaggle opencv-python tqdm pillow pandas scikit-learn\n",
    "\n",
    "print(\"\\nâœ… ì„¤ì¹˜ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Œ Step 4: Kaggle API ì„¤ì •\n",
    "\n",
    "**âš ï¸ ì¤‘ìš”: ì²˜ìŒ ì‹¤í–‰ ì‹œ kaggle.json ì—…ë¡œë“œ í•„ìš”**\n",
    "\n",
    "### Kaggle API í† í° ë°›ëŠ” ë°©ë²•:\n",
    "1. https://www.kaggle.com/settings ì ‘ì†\n",
    "2. \"Create New API Token\" í´ë¦­\n",
    "3. ë‹¤ìš´ë¡œë“œëœ `kaggle.json` íŒŒì¼ì„ Google Driveì— ì—…ë¡œë“œ:\n",
    "   - ìœ„ì¹˜: `/MyDrive/HAI_Deepfake/kaggle.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# kaggle.json ê²½ë¡œ\n",
    "kaggle_json_drive = DRIVE_ROOT / \"kaggle.json\"\n",
    "kaggle_json_local = Path.home() / \".kaggle\" / \"kaggle.json\"\n",
    "\n",
    "if kaggle_json_drive.exists():\n",
    "    print(\"âœ… kaggle.json íŒŒì¼ ë°œê²¬!\")\n",
    "    \n",
    "    # ~/.kaggle ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "    kaggle_json_local.parent.mkdir(exist_ok=True)\n",
    "    \n",
    "    # ë³µì‚¬\n",
    "    shutil.copy(kaggle_json_drive, kaggle_json_local)\n",
    "    \n",
    "    # ê¶Œí•œ ì„¤ì •\n",
    "    os.chmod(kaggle_json_local, 0o600)\n",
    "    \n",
    "    print(\"âœ… Kaggle API ì„¤ì • ì™„ë£Œ!\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸\n",
    "    !kaggle datasets list --max-size 1000 | head -5\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ kaggle.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "    print(f\"\\nğŸ“ ë‹¤ìŒ ê²½ë¡œì— ì—…ë¡œë“œí•˜ì„¸ìš”:\")\n",
    "    print(f\"   {kaggle_json_drive}\")\n",
    "    print(f\"\\nğŸ’¡ Kaggle API í† í° ë°›ëŠ” ë°©ë²•:\")\n",
    "    print(f\"   1. https://www.kaggle.com/settings\")\n",
    "    print(f\"   2. 'Create New API Token' í´ë¦­\")\n",
    "    print(f\"   3. ë‹¤ìš´ë¡œë“œëœ kaggle.jsonì„ ìœ„ ê²½ë¡œì— ì—…ë¡œë“œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Œ Step 5: ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ\n",
    "\n",
    "### ì¶”ì²œ ë°ì´í„°ì…‹:\n",
    "1. **FaceForensics++** (~10GB) - ì†Œê·œëª¨ ì‹œì‘ìš©\n",
    "2. **CelebA** (~1.5GB) - Real ì–¼êµ´ ì¶”ê°€ìš©\n",
    "3. **DFDC** (~470GB) - ëŒ€ê·œëª¨ (ì„ íƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰\n",
    "%run scripts/download_datasets.py\n",
    "\n",
    "# downloader ê°ì²´ ì‚¬ìš© ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜µì…˜ 1: FaceForensics++ ë‹¤ìš´ë¡œë“œ (ì†Œê·œëª¨ ì‹œì‘ - ì¶”ì²œ)\n",
    "print(\"ğŸ“¥ FaceForensics++ ë‹¤ìš´ë¡œë“œ ì‹œì‘...\")\n",
    "print(\"â±ï¸  ì˜ˆìƒ ì‹œê°„: 10~30ë¶„ (ë„¤íŠ¸ì›Œí¬ ì†ë„ì— ë”°ë¼ ë‹¤ë¦„)\\n\")\n",
    "\n",
    "# downloader.download_dataset(\n",
    "#     dataset_name=\"sorokin/faceforensics\",\n",
    "#     output_name=\"faceforensics\"\n",
    "# )\n",
    "\n",
    "print(\"\\nâš ï¸  ì£¼ì„ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜µì…˜ 2: CelebA ì¶”ê°€ ë‹¤ìš´ë¡œë“œ (Real ì´ë¯¸ì§€ìš©)\n",
    "# downloader.download_dataset(\n",
    "#     dataset_name=\"jessicali9530/celeba-dataset\",\n",
    "#     output_name=\"celeba\"\n",
    "# )\n",
    "\n",
    "print(\"âš ï¸  Real ì´ë¯¸ì§€ê°€ ë” í•„ìš”í•˜ë©´ ì£¼ì„ í•´ì œí•˜ê³  ì‹¤í–‰í•˜ì„¸ìš”!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜„ì¬ ë‹¤ìš´ë¡œë“œëœ ë°ì´í„°ì…‹ í™•ì¸\n",
    "info = downloader.get_dataset_info()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š ë‹¤ìš´ë¡œë“œëœ ë°ì´í„°ì…‹\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if info:\n",
    "    for name, details in info.items():\n",
    "        print(f\"âœ… {name}:\")\n",
    "        print(f\"   ğŸ“ ê²½ë¡œ: {details['path']}\")\n",
    "        print(f\"   ğŸ“„ íŒŒì¼: {details['file_count']:,d} ê°œ\")\n",
    "        print(f\"   ğŸ’¾ í¬ê¸°: {details['size_gb']:.2f} GB\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"ğŸ“­ ì•„ì§ ë‹¤ìš´ë¡œë“œëœ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    print(\"ìœ„ ì…€ì—ì„œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”!\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Œ Step 6: ë¹„ë””ì˜¤ â†’ ì´ë¯¸ì§€ ë³€í™˜\n",
    "\n",
    "ë‹¤ìš´ë¡œë“œëœ ë¹„ë””ì˜¤ì—ì„œ í”„ë ˆì„ì„ ì¶”ì¶œí•˜ì—¬ ì´ë¯¸ì§€ë¡œ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë ˆì„ ì¶”ì¶œ ì„¤ì •\n",
    "INPUT_DATASET = str(DRIVE_ROOT / \"datasets\" / \"faceforensics\")  # ì…ë ¥ ë°ì´í„°ì…‹\n",
    "OUTPUT_DIR = str(DRIVE_ROOT / \"train_data\")  # ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
    "MAX_FRAMES = 30  # ë¹„ë””ì˜¤ë‹¹ ì¶”ì¶œí•  í”„ë ˆì„ ìˆ˜\n",
    "MAX_VIDEOS = None  # í…ŒìŠ¤íŠ¸ìš©: ìˆ«ì ì§€ì • ì‹œ í•´ë‹¹ ê°œìˆ˜ë§Œ ì²˜ë¦¬ (ì˜ˆ: 10)\n",
    "\n",
    "print(f\"ğŸ“‚ ì…ë ¥: {INPUT_DATASET}\")\n",
    "print(f\"ğŸ“ ì¶œë ¥: {OUTPUT_DIR}\")\n",
    "print(f\"âš™ï¸  ì„¤ì •: ë¹„ë””ì˜¤ë‹¹ {MAX_FRAMES} í”„ë ˆì„\")\n",
    "\n",
    "if MAX_VIDEOS:\n",
    "    print(f\"âš ï¸  í…ŒìŠ¤íŠ¸ ëª¨ë“œ: {MAX_VIDEOS}ê°œ ë¹„ë””ì˜¤ë§Œ ì²˜ë¦¬\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í”„ë ˆì„ ì¶”ì¶œ ì‹¤í–‰\n",
    "!python scripts/extract_frames.py \\\n",
    "    --input \"{INPUT_DATASET}\" \\\n",
    "    --output \"{OUTPUT_DIR}\" \\\n",
    "    --max-frames {MAX_FRAMES} \\\n",
    "    --sample-method uniform \\\n",
    "    --quality 95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Œ Step 7: ì†Œê·œëª¨ ë°ì´í„°ì…‹ ìƒì„± (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)\n",
    "\n",
    "ì „ì²´ ë°ì´í„°ë¡œ í•™ìŠµí•˜ê¸° ì „ì— ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¨¼ì € í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì†Œê·œëª¨ ë°ì´í„°ì…‹ ì„¤ì •\n",
    "SMALL_INPUT = str(DRIVE_ROOT / \"train_data\")\n",
    "SMALL_OUTPUT = str(DRIVE_ROOT / \"train_data_small\")\n",
    "NUM_SAMPLES = 1000  # í´ë˜ìŠ¤ë‹¹ ìƒ˜í”Œ ìˆ˜ (Real 1000 + Fake 1000 = ì´ 2000)\n",
    "\n",
    "print(f\"ğŸ“‚ ì›ë³¸: {SMALL_INPUT}\")\n",
    "print(f\"ğŸ“ ì¶œë ¥: {SMALL_OUTPUT}\")\n",
    "print(f\"ğŸ“Š ìƒ˜í”Œ: í´ë˜ìŠ¤ë‹¹ {NUM_SAMPLES:,d} ê°œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì†Œê·œëª¨ ë°ì´í„°ì…‹ ìƒì„±\n",
    "!python scripts/create_small_dataset.py \\\n",
    "    --input \"{SMALL_INPUT}\" \\\n",
    "    --output \"{SMALL_OUTPUT}\" \\\n",
    "    --num-samples {NUM_SAMPLES} \\\n",
    "    --seed 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ“Œ Step 8: ë°ì´í„° ê²€ì¦\n",
    "\n",
    "ìƒì„±ëœ ë°ì´í„°ì…‹ì„ í™•ì¸í•˜ê³  ìƒ˜í”Œ ì´ë¯¸ì§€ë¥¼ ë´…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "def show_samples(data_dir, num_samples=6):\n",
    "    \"\"\"ë°ì´í„°ì…‹ ìƒ˜í”Œ ì‹œê°í™”\"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(15, 5))\n",
    "    fig.suptitle(f\"Dataset Samples: {data_dir}\", fontsize=16)\n",
    "    \n",
    "    for idx, label in enumerate([\"real\", \"fake\"]):\n",
    "        label_dir = data_path / label\n",
    "        images = list(label_dir.glob(\"*.jpg\"))\n",
    "        \n",
    "        if len(images) == 0:\n",
    "            print(f\"âš ï¸  {label} ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            continue\n",
    "        \n",
    "        samples = random.sample(images, min(num_samples, len(images)))\n",
    "        \n",
    "        for i, img_path in enumerate(samples):\n",
    "            img = Image.open(img_path)\n",
    "            axes[idx, i].imshow(img)\n",
    "            axes[idx, i].axis('off')\n",
    "            axes[idx, i].set_title(f\"{label.upper()}\", fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ì†Œê·œëª¨ ë°ì´í„°ì…‹ ìƒ˜í”Œ ë³´ê¸°\n",
    "show_samples(SMALL_OUTPUT, num_samples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ í†µê³„\n",
    "def print_dataset_stats(data_dir):\n",
    "    \"\"\"ë°ì´í„°ì…‹ í†µê³„ ì¶œë ¥\"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"ğŸ“Š ë°ì´í„°ì…‹ í†µê³„: {data_dir}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    total = 0\n",
    "    for label in [\"real\", \"fake\"]:\n",
    "        label_dir = data_path / label\n",
    "        if label_dir.exists():\n",
    "            images = list(label_dir.glob(\"*.jpg\"))\n",
    "            images += list(label_dir.glob(\"*.png\"))\n",
    "            count = len(images)\n",
    "            total += count\n",
    "            print(f\"  {label.upper():5s}: {count:8,d} ì´ë¯¸ì§€\")\n",
    "    \n",
    "    print(f\"  {'TOTAL':5s}: {total:8,d} ì´ë¯¸ì§€\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# ì „ì²´ ë°ì´í„°ì…‹\n",
    "print_dataset_stats(SMALL_INPUT)\n",
    "\n",
    "# ì†Œê·œëª¨ ë°ì´í„°ì…‹\n",
    "print_dataset_stats(SMALL_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## âœ… ì™„ë£Œ!\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„:\n",
    "1. **ì†Œê·œëª¨ë¡œ í•™ìŠµ ì‹œì‘**: `train_colab.ipynb` ì‹¤í–‰\n",
    "2. **ë°ì´í„° ê²½ë¡œ ì„¤ì •**: \n",
    "   - ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸: `/content/drive/MyDrive/HAI_Deepfake/train_data_small`\n",
    "   - ì „ì²´ í•™ìŠµ: `/content/drive/MyDrive/HAI_Deepfake/train_data`\n",
    "\n",
    "### ì €ì¥ëœ ìœ„ì¹˜:\n",
    "- ğŸ“ Google Drive: `/MyDrive/HAI_Deepfake/`\n",
    "- ğŸ¬ ì›ë³¸ ë¹„ë””ì˜¤: `datasets/`\n",
    "- ğŸ–¼ï¸ ì „ì²´ ì´ë¯¸ì§€: `train_data/`\n",
    "- ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°: `train_data_small/`\n",
    "\n",
    "### íŒ:\n",
    "- Colab ì„¸ì…˜ì´ ëŠê²¨ë„ Google Drive ë°ì´í„°ëŠ” ì•ˆì „!\n",
    "- ë‹¤ìŒ ë²ˆì—ëŠ” Step 1ë¶€í„° ë°”ë¡œ ì‹œì‘ ê°€ëŠ¥\n",
    "- ë” ë§ì€ ë°ì´í„°ê°€ í•„ìš”í•˜ë©´ Step 5ë¡œ ëŒì•„ê°€ì„œ ì¶”ê°€ ë‹¤ìš´ë¡œë“œ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
