{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAI Deepfake - Ultimate Inference Pipeline (Top-K Strategy)\n",
    "\n",
    "**ì „ëµ**: \n",
    "1. ë¡œì»¬ ì „ì²˜ë¦¬ ë°ì´í„° (RetinaFace Crop) ì‚¬ìš©\n",
    "2. EfficientNet-V2-M (480x480) ìˆœì •/í•™ìŠµ ëª¨ë¸ ìë™ ì „í™˜\n",
    "3. **Top-K Average**: ë¹„ë””ì˜¤ 30í”„ë ˆì„ ì¤‘ 'ê°€ì§œ' í™•ë¥ ì´ ë†’ì€ ìƒìœ„ 5ì¥ì˜ í‰ê· ê°’ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. í™˜ê²½ ì„¤ì • ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n",
    "!nvidia-smi\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"Install Dependencies...\")\n",
    "# Pillow ì—…ê·¸ë ˆì´ë“œ í¬í•¨ (ImportError ë°©ì§€)\n",
    "!pip install -q timm facenet-pytorch albumentations pillow --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.5. ì½”ë“œ ë™ê¸°í™” (Git Sync) - src í´ë” ê°€ì ¸ì˜¤ê¸°\n",
    "import os\n",
    "\n",
    "REPO_URL = \"https://github.com/CBottle/HAI_Deepfake.git\"\n",
    "PROJECT_DIR = \"/content/HAI_Deepfake\"\n",
    "BRANCH = \"HK\" \n",
    "\n",
    "if os.path.exists(PROJECT_DIR):\n",
    "    print(f\"Pulling latest code from {BRANCH}...\")\n",
    "    %cd {PROJECT_DIR}\n",
    "    !git fetch --all\n",
    "    !git reset --hard origin/{BRANCH}\n",
    "else:\n",
    "    print(f\"Cloning project ({BRANCH})...\")\n",
    "    !git clone -b {BRANCH} {REPO_URL} {PROJECT_DIR}\n",
    "    %cd {PROJECT_DIR}\n",
    "\n",
    "print(\"âœ… ì½”ë“œ ì¤€ë¹„ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 2. ë°ì´í„° ì¤€ë¹„\n",
    "DRIVE_ZIP_PATH = '/content/drive/MyDrive/HAI_Deepfake/processed_test_data.zip'\n",
    "LOCAL_DATA_DIR = '/content/processed_test_data'\n",
    "\n",
    "if not os.path.exists(LOCAL_DATA_DIR):\n",
    "    print(\"ğŸ“‚ ë°ì´í„° ë³µì‚¬ ë° ì••ì¶• í•´ì œ ì¤‘...\")\n",
    "    os.makedirs(LOCAL_DATA_DIR, exist_ok=True)\n",
    "    !cp {DRIVE_ZIP_PATH} /content/data.zip\n",
    "    !unzip -q /content/data.zip -d {LOCAL_DATA_DIR}\n",
    "    print(\"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!\")\n",
    "else:\n",
    "    print(\"âœ… ë°ì´í„°ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# ëª¨ë¸ ê²½ë¡œ\n",
    "MODEL_PATH = '/content/drive/MyDrive/HAI_Deepfake/checkpoints/best_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. ì¶”ë¡  ì½”ë“œ ì‹¤í–‰ (Inference Engine)\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import timm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "from timm.data import resolve_data_config, create_transform\n",
    "\n",
    "# src ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€\n",
    "sys.path.append(\"/content/HAI_Deepfake\")\n",
    "try:\n",
    "    from src.models import load_model\n",
    "    print(\"âœ… src.models loaded successfully!\")\n",
    "except ImportError:\n",
    "    print(\"âŒ Failed to load src.models. Please check Step 1.5.\")\n",
    "\n",
    "# ì„¤ì •ê°’ ì¬ì •ì˜\n",
    "MODEL_NAME = \"tf_efficientnetv2_m.in21k\"\n",
    "MODEL_PATH = '/content/drive/MyDrive/HAI_Deepfake/checkpoints/best_model.pt'\n",
    "LOCAL_DATA_DIR = '/content/processed_test_data'\n",
    "OUTPUT_CSV = \"submission.csv\"\n",
    "\n",
    "class ProcessedDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.transform = transform\n",
    "        self.items = sorted([item for item in self.data_dir.glob('*') if not item.name.startswith('.')])\n",
    "        \n",
    "        if len(self.items) == 1 and self.items[0].is_dir():\n",
    "            print(f\"ğŸ” í•˜ìœ„ í´ë” ê°ì§€: {self.items[0].name} ë¡œ ì§„ì…í•©ë‹ˆë‹¤.\")\n",
    "            self.data_dir = self.items[0]\n",
    "            self.items = sorted([item for item in self.data_dir.glob('*') if not item.name.startswith('.')])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_path = self.items[idx]\n",
    "        pixel_values_list = []\n",
    "        \n",
    "        if item_path.is_dir():\n",
    "            img_paths = sorted(list(item_path.glob('*.jpg')))\n",
    "            for p in img_paths:\n",
    "                img = Image.open(p).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    pixel_values_list.append(self.transform(img))\n",
    "        else:\n",
    "            img = Image.open(item_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                pixel_values_list.append(self.transform(img))\n",
    "\n",
    "        if not pixel_values_list:\n",
    "            return torch.zeros(1, 3, 480, 480), item_path.name\n",
    "\n",
    "        return torch.stack(pixel_values_list), item_path.name\n",
    "\n",
    "def run_inference():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # ëª¨ë¸ ë¡œë“œ (ë¶„ê¸° ì²˜ë¦¬)\n",
    "    if os.path.exists(MODEL_PATH):\n",
    "        print(f\"âœ… Loading Fine-tuned Model (6-channel SRM): {MODEL_PATH}\")\n",
    "        # 6ì±„ë„ DeepfakeDetector ë¡œë“œ\n",
    "        model = load_model(MODEL_PATH, MODEL_NAME, device)\n",
    "        default_cfg = model.model.default_cfg\n",
    "    else:\n",
    "        print(f\"âš ï¸ Loading Pure Pretrained Model (3-channel): {MODEL_NAME}\")\n",
    "        # 3ì±„ë„ ìˆœì • ëª¨ë¸ ë¡œë“œ\n",
    "        model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=2).to(device)\n",
    "        default_cfg = model.default_cfg\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    # Transform\n",
    "    data_config = resolve_data_config(default_cfg, model=model)\n",
    "    print(f\"Data Config: {data_config}\")\n",
    "    transform = create_transform(**data_config)\n",
    "\n",
    "    # DataLoader\n",
    "    dataset = ProcessedDataset(LOCAL_DATA_DIR, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2)\n",
    "\n",
    "    # ì¶”ë¡ \n",
    "    results = {}\n",
    "    print(f\"ğŸš€ Starting inference on {len(dataset)} items...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for pixel_values, filenames in tqdm(dataloader):\n",
    "            pixel_values = pixel_values.squeeze(0).to(device)\n",
    "            filename = filenames[0]\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(pixel_values)\n",
    "                if hasattr(outputs, 'logits'):\n",
    "                    logits = outputs.logits\n",
    "                else:\n",
    "                    logits = outputs\n",
    "                \n",
    "                probs = F.softmax(logits, dim=1)[:, 1] # Fake í™•ë¥ \n",
    "            \n",
    "            k = min(5, len(probs))\n",
    "            top_k_probs, _ = torch.topk(probs, k)\n",
    "            final_prob = float(top_k_probs.mean().cpu().item())\n",
    "            \n",
    "            clean_filename = filename.replace('_frames', '')\n",
    "            results[clean_filename] = final_prob\n",
    "\n",
    "    return results\n",
    "\n",
    "results = run_inference()\n",
    "\n",
    "submission = pd.DataFrame(list(results.items()), columns=['filename', 'prob'])\n",
    "submission.to_csv(OUTPUT_CSV, index=False)\n",
    "print(f\"âœ… Submission saved: {OUTPUT_CSV}\")\n",
    "\n",
    "BACKUP_PATH = '/content/drive/MyDrive/HAI_Deepfake/submission_final_srm.csv'\n",
    "!cp {OUTPUT_CSV} {BACKUP_PATH}\n",
    "print(f\"ğŸ’¾ Drive Backup: {BACKUP_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}