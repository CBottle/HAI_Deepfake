"""
HAI Deepfake Detection - Training Script

í•™ìŠµ ì½”ë“œ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
"""

import argparse
import os
import shutil
from pathlib import Path

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import ViTImageProcessor
from tqdm import tqdm
from sklearn.metrics import roc_auc_score
from torchvision import transforms
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np

from src.models import DeepfakeDetector
from src.dataset import DeepfakeDataset
from src.utils import (
    set_seed,
    load_config,
    save_checkpoint,
    load_checkpoint,
    get_device,
    AverageMeter
)


def parse_args():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='Train Deepfake Detection Model')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                        help='Path to config file')
    parser.add_argument('--resume', type=str, default=None,
                        help='Path to checkpoint to resume from')
    parser.add_argument('--debug', action='store_true',
                        help='Debug mode (small dataset)')
    parser.add_argument('--unfreeze', action='store_true',
                        help='Stage 2: Unfreeze backbone for full fine-tuning')
    return parser.parse_args()

def train_epoch(model, dataloader, criterion, optimizer, device, scaler=None):
    """
    í•œ ì—í¬í¬ í•™ìŠµ

    Args:
        model: í•™ìŠµí•  ëª¨ë¸
        dataloader: ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        optimizer: ì˜µí‹°ë§ˆì´ì €
        device: ë””ë°”ì´ìŠ¤
        scaler: GradScaler (Mixed Precision)

    Returns:
        í‰ê·  ì†ì‹¤
    """
    model.train()
    loss_meter = AverageMeter()

    pbar = tqdm(dataloader, desc='Training')
    for i, batch in enumerate(pbar):
        pixel_values = batch['pixel_values'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        # Mixed Precision Training
        if scaler is not None: # (í˜„ì¬ ì½”ë“œì—” scalerê°€ ì—†ì§€ë§Œ êµ¬ì¡° ìœ ì§€)
            pass
        
        logits = model(pixel_values)
        loss = criterion(logits, labels)
        
        # [ê¸´ê¸‰ ë””ë²„ê¹…] ì²« ë²ˆì§¸ ë°°ì¹˜ì˜ ìƒíƒœ í™•ì¸
        if i == 0:
            print(f"\n[Debug] Logits Range: Min={logits.min().item():.4f}, Max={logits.max().item():.4f}")
            print(f"[Debug] Labels Sample: {labels[:10].cpu().numpy()}")
            
            # ë¼ë²¨ ë°˜ì „ í…ŒìŠ¤íŠ¸
            loss_inverted = criterion(logits, 1 - labels)
            print(f"[Debug] Original Loss: {loss.item():.4f} vs Inverted Label Loss: {loss_inverted.item():.4f}")
            
            if loss_inverted.item() < loss.item():
                print("ğŸš¨ [WARNING] ë¼ë²¨ì´ ë°˜ëŒ€ì¼ í™•ë¥ ì´ ë§¤ìš° ë†’ìŠµë‹ˆë‹¤! (Inverted Lossê°€ ë” ë‚®ìŒ)")

        loss.backward()
        optimizer.step()

        loss_meter.update(loss.item(), pixel_values.size(0))
        pbar.set_postfix({'loss': f'{loss_meter.avg:.4f}'})

    return loss_meter.avg


def validate(model, dataloader, criterion, device):
    """
    ê²€ì¦

    Args:
        model: ëª¨ë¸
        dataloader: ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        device: ë””ë°”ì´ìŠ¤

    Returns:
        í‰ê·  ì†ì‹¤, ROC-AUC
    """
    model.eval()
    loss_meter = AverageMeter()

    all_probs = []
    all_labels = []

    with torch.no_grad():
        pbar = tqdm(dataloader, desc='Validation')
        for batch in pbar:
            pixel_values = batch['pixel_values'].to(device)
            labels = batch['labels'].to(device)

            logits = model(pixel_values)
            loss = criterion(logits, labels)

            probs = torch.softmax(logits, dim=1)[:, 1]  # Fake í™•ë¥ 

            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            loss_meter.update(loss.item(), pixel_values.size(0))
            pbar.set_postfix({'loss': f'{loss_meter.avg:.4f}'})

    # ROC-AUC ê³„ì‚°
    auc = roc_auc_score(all_labels, all_probs)

    return loss_meter.avg, auc


# í•™ìŠµ ë°ì´í„°ë¥¼ ìœ„í•œ 'ìˆœí•œ ë§›' ì¦ê°• ì„¤ì • (Soft Augmentation)
# í™”ì§ˆì„ ì†ìƒì‹œí‚¤ì§€ ì•Šê³  í˜•íƒœì˜ ë‹¤ì–‘ì„±ë§Œ í™•ë³´í•©ë‹ˆë‹¤.
soft_transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.3),
    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.2),
])


def main():
    """ìˆœí•œ ë§› ì¦ê°• ë²„ì „ - ê³ ë“ì  Fine-tuningìš©"""
    args = parse_args()
    config = load_config(args.config)
    set_seed(config['experiment']['seed'])

    # 1. ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = get_device() 
    print(f"ğŸš€ í•™ìŠµ ì‹œì‘! ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # ëª¨ë¸ ì´ˆê¸°í™”
    # ProcessorëŠ” Hugging Faceì˜ ViTìš©ì„ ë¹Œë ¤ ì”€ (EfficientNetë„ 224x224 Normalizeë¼ í˜¸í™˜ë¨)
    processor = ViTImageProcessor.from_pretrained("google/vit-base-patch16-224")
    model = DeepfakeDetector(
        model_name=config['model']['name'],
        num_classes=config['model']['num_classes'],
        pretrained=config['model']['pretrained']
    ).to(device)

    # 2. ë°ì´í„° ìƒ˜í”Œë§ (Real:Fake = 1:1 ë°¸ëŸ°ì‹±)
    import pandas as pd
    from sklearn.model_selection import GroupShuffleSplit # ê·¸ë£¹ ìŠ¤í”Œë¦¿ ì¶”ê°€

    train_csv_path = config['data']['train_csv']
    if os.path.exists(train_csv_path):
        full_df = pd.read_csv(train_csv_path)
        
        # í´ë˜ìŠ¤ ë¶„ë¦¬
        df_real = full_df[full_df['label'] == 0]
        df_fake = full_df[full_df['label'] == 1]
        
        target_per_class = 15000
        
        # ê° í´ë˜ìŠ¤ì—ì„œ 1.5ë§Œ ì¥ì”© ìƒ˜í”Œë§
        s_real = df_real.sample(n=min(target_per_class, len(df_real)), random_state=42)
        s_fake = df_fake.sample(n=min(target_per_class, len(df_fake)), random_state=42)
        
        # ë°ì´í„° ë³‘í•©
        balanced_df = pd.concat([s_real, s_fake]).reset_index(drop=True)
        
        # [Data Leakage ë°©ì§€] ë¹„ë””ì˜¤ ID ì¶”ì¶œ ë° ê·¸ë£¹ ìŠ¤í”Œë¦¿
        # íŒŒì¼ëª… ì˜ˆì‹œ: 'video_01_frame0.jpg', 'aomwayen.mp4_frame10.jpg'
        # ì „ëµ: ë’¤ì—ì„œ ì²« ë²ˆì§¸ '_' ê¸°ì¤€ ì•ë¶€ë¶„ì„ ë¹„ë””ì˜¤ IDë¡œ ê°„ì£¼
        balanced_df['video_id'] = balanced_df['filename'].apply(lambda x: x.rsplit('_', 1)[0] if '_' in x else x)
        
        gss = GroupShuffleSplit(n_splits=1, test_size=0.1, random_state=42)
        train_idx, val_idx = next(gss.split(balanced_df, groups=balanced_df['video_id']))
        
        train_df = balanced_df.iloc[train_idx]
        val_df = balanced_df.iloc[val_idx]
        
        print(f"ğŸ“Š [ê·¸ë£¹ ìŠ¤í”Œë¦¿] ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: ì´ {len(balanced_df)}ì¥")
        print(f"   - í•™ìŠµ(Train): {len(train_df)}ì¥ (Videos: {train_df['video_id'].nunique()})")
        print(f"   - ê²€ì¦(Val):   {len(val_df)}ì¥ (Videos: {val_df['video_id'].nunique()})")
        
        # ì„ì‹œ íŒŒì¼ ì €ì¥
        train_df.to_csv("temp_train.csv", index=False)
        val_df.to_csv("temp_val.csv", index=False)
    else:
        raise FileNotFoundError(f"âš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_csv_path}")

    # 3. ë°ì´í„°ì…‹ ë° ë¡œë”
    train_dataset = DeepfakeDataset(
        csv_path="temp_train.csv",
        img_dir=config['data']['img_dir'],
        processor=processor,
        num_frames=config['data']['num_frames'],
        transform=soft_transform # ìˆœí•œë§› ì ìš©
    )
    
    val_dataset = DeepfakeDataset(
        csv_path="temp_val.csv",
        img_dir=config['data']['img_dir'],
        processor=processor,
        num_frames=config['data']['num_frames'],
        transform=None 
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=True,
        num_workers=config['training']['num_workers'],
        pin_memory=True if device == 'cuda' else False
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=False,
        num_workers=config['training']['num_workers'],
        pin_memory=True if device == 'cuda' else False
    )

    # [Training Stage Selection]
    if args.unfreeze:
        # [Stage 2: Full Fine-tuning with Differential LR]
        print("ğŸ”“ [Stage 2] Unfreezing All Layers with Differential LR...")
        for param in model.parameters():
            param.requires_grad = True
            
        # íŒŒë¼ë¯¸í„° ê·¸ë£¹ ë¶„ë¦¬ (Backbone vs Head)
        backbone_params = []
        head_params = []
        
        # Head ì´ë¦„ ì°¾ê¸° (timm í˜¸í™˜)
        head_name = 'classifier' if hasattr(model.model, 'classifier') else 'fc' if hasattr(model.model, 'fc') else 'head'
        
        for name, param in model.named_parameters():
            if head_name in name:
                head_params.append(param)
            else:
                backbone_params.append(param)
        
        # ì°¨ë“± í•™ìŠµë¥  ì ìš©
        optimizer = optim.AdamW([
            {'params': backbone_params, 'lr': 1e-6}, # ëª¸í†µ: ì§€ì‹ ë³´ì¡´ (ì•„ì£¼ ì‚´ì‚´)
            {'params': head_params, 'lr': 1e-4}      # ë¨¸ë¦¬: ë¹ ë¥¸ ì ì‘
        ], weight_decay=0.01)
    else:
        # [Stage 1: SRM Warmup]
        print("ğŸ”’ [Stage 1] Freezing Backbone Body for SRM Adaptation...")
        
        # 1. ì „ì²´ ë°±ë³¸ Freeze
        for param in model.model.parameters():
            param.requires_grad = False
            
        # 2. ì²« ë²ˆì§¸ ë ˆì´ì–´ (conv_stem) Unfreeze
        for param in model.model.conv_stem.parameters():
            param.requires_grad = True
            
        # 3. ë¶„ë¥˜ê¸° (classifier) Unfreeze
        head = getattr(model.model, 'classifier', getattr(model.model, 'fc', getattr(model.model, 'head', None)))
        if head:
            for param in head.parameters():
                param.requires_grad = True

        # í•™ìŠµí•  íŒŒë¼ë¯¸í„°ë§Œ ê³¨ë¼ì„œ Optimizerì— ì „ë‹¬ (ë†’ì€ LR)
        trainable_params = [p for p in model.parameters() if p.requires_grad]
        optimizer = optim.AdamW(trainable_params, lr=1e-3, weight_decay=0.01)
    
    # ìŠ¤ì¼€ì¤„ëŸ¬: ì›œì—…(Warmup) í›„ ì½”ì‚¬ì¸ ì–´ë‹ë§
    from torch.optim.lr_scheduler import LinearLR, CosineAnnealingLR, SequentialLR
    
    warmup_scheduler = LinearLR(optimizer, start_factor=0.1, end_factor=1.0, total_iters=len(train_loader))
    cosine_scheduler = CosineAnnealingLR(optimizer, T_max=config['training']['epochs'] * len(train_loader))
    
    scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, cosine_scheduler], milestones=[len(train_loader)])
    criterion = torch.nn.CrossEntropyLoss()

    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (Resume)
    start_epoch = 0
    best_auc = 0.0
    
    if args.resume:
        if os.path.isfile(args.resume):
            print(f"ğŸ”„ Resuming from checkpoint: {args.resume}")
            checkpoint = load_checkpoint(args.resume, model, optimizer, device)
            start_epoch = checkpoint['epoch'] + 1
            if 'val_auc' in checkpoint:
                best_auc = checkpoint['val_auc']
            print(f"   -> Resuming from Epoch {start_epoch+1}")
        else:
            print(f"âš ï¸ Checkpoint not found: {args.resume}")

    # í•™ìŠµ ë£¨í”„
    print(f"\n=== Start Fine-tuning (Total Epochs: {config['training']['epochs']}) ===")
    ckpt_dir = config['training']['experiment']['output_dir']
    
    for epoch in range(start_epoch, config['training']['epochs']):
        # 1. í•™ìŠµ
        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)
        
        # 2. ê²€ì¦
        val_loss, val_auc = validate(model, val_loader, criterion, device)
        
        print(f"Epoch {epoch+1}/{config['training']['epochs']} | "
              f"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val AUC: {val_auc:.4f}")
        
        # 3. ì²´í¬í¬ì¸íŠ¸ ì €ì¥
        is_best = val_auc > best_auc
        if is_best:
            best_auc = val_auc
            print(f"ğŸ† Best AUC Updated: {best_auc:.4f}")
        
        save_checkpoint(
            model, 
            optimizer, 
            epoch, 
            val_auc, 
            checkpoint_dir=ckpt_dir, 
            is_best=is_best
        )

        scheduler.step()

if __name__ == '__main__':
    main()
