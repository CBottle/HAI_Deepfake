"""
HAI Deepfake Detection - Training Script

í•™ìŠµ ì½”ë“œ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
"""

import argparse
import os
import shutil
from pathlib import Path

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import ViTImageProcessor
from tqdm import tqdm
from sklearn.metrics import roc_auc_score
from torchvision import transforms
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np

from src.models import DeepfakeDetector
from src.dataset import DeepfakeDataset
from src.utils import (
    set_seed,
    load_config,
    save_checkpoint,
    load_checkpoint,
    get_device,
    AverageMeter
)


def parse_args():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='Train Deepfake Detection Model')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                        help='Path to config file')
    parser.add_argument('--resume', type=str, default=None,
                        help='Path to checkpoint to resume from')
    parser.add_argument('--debug', action='store_true',
                        help='Debug mode (small dataset)')
    return parser.parse_args()


def train_epoch(model, dataloader, criterion, optimizer, device, scaler=None):
    """
    í•œ ì—í¬í¬ í•™ìŠµ

    Args:
        model: í•™ìŠµí•  ëª¨ë¸
        dataloader: ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        optimizer: ì˜µí‹°ë§ˆì´ì €
        device: ë””ë°”ì´ìŠ¤
        scaler: GradScaler (Mixed Precision)

    Returns:
        í‰ê·  ì†ì‹¤
    """
    model.train()
    loss_meter = AverageMeter()

    pbar = tqdm(dataloader, desc='Training')
    for batch in pbar:
        pixel_values = batch['pixel_values'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        # Mixed Precision Training
        if scaler is not None:
            with torch.cuda.amp.autocast():
                logits = model(pixel_values)
                loss = criterion(logits, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            logits = model(pixel_values)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()

        loss_meter.update(loss.item(), pixel_values.size(0))
        pbar.set_postfix({'loss': f'{loss_meter.avg:.4f}'})

    return loss_meter.avg


def validate(model, dataloader, criterion, device):
    """
    ê²€ì¦

    Args:
        model: ëª¨ë¸
        dataloader: ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        device: ë””ë°”ì´ìŠ¤

    Returns:
        í‰ê·  ì†ì‹¤, ROC-AUC
    """
    model.eval()
    loss_meter = AverageMeter()

    all_probs = []
    all_labels = []

    with torch.no_grad():
        pbar = tqdm(dataloader, desc='Validation')
        for batch in pbar:
            pixel_values = batch['pixel_values'].to(device)
            labels = batch['labels'].to(device)

            logits = model(pixel_values)
            loss = criterion(logits, labels)

            probs = torch.softmax(logits, dim=1)[:, 1]  # Fake í™•ë¥ 

            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            loss_meter.update(loss.item(), pixel_values.size(0))
            pbar.set_postfix({'loss': f'{loss_meter.avg:.4f}'})

    # ROC-AUC ê³„ì‚°
    auc = roc_auc_score(all_labels, all_probs)

    return loss_meter.avg, auc


def main():
    args = parse_args()
    config = load_config(args.config)
    set_seed(config['experiment']['seed'])

    # 1. CPU ê°•ì œ ì„¤ì • (í…ŒìŠ¤íŠ¸ìš©)
    device = torch.device('cpu') 
    print(f"Device forced to: {device}")

    # 2. ë°ì´í„° 100ì¥ ìƒ˜í”Œë§ ë° ì„ì‹œ CSV ìƒì„±
    import pandas as pd
    full_df = pd.read_csv(config['data']['train_csv'])
    
    # Labelì´ 0(Real), 1(Fake)ë¼ê³  ê°€ì • (ë°ì´í„°ì— ë§ì¶° í™•ì¸í•´!)
    df_real = full_df[full_df['label'] == 0].sample(n=min(50, len(full_df[full_df['label']==0])), random_state=42)
    df_fake = full_df[full_df['label'] == 1].sample(n=min(50, len(full_df[full_df['label']==1])), random_state=42)
    tiny_df = pd.concat([df_real, df_fake]).reset_index(drop=True)
    
    # ì„ì‹œ CSV ì €ì¥ (DeepfakeDatasetì´ ê²½ë¡œë¥¼ ë°›ìœ¼ë¯€ë¡œ)
    tiny_csv_path = 'config/tiny_train.csv'
    tiny_df.to_csv(tiny_csv_path, index=False)
    print(f"âœ… Tiny Dataset ìƒì„± ì™„ë£Œ (100ì¥): {tiny_csv_path}")

    # ëª¨ë¸ ì´ˆê¸°í™”
    processor = ViTImageProcessor.from_pretrained(config['model']['name'])
    model = DeepfakeDetector(
        model_name=config['model']['name'],
        num_classes=config['model']['num_classes'],
        pretrained=config['model']['pretrained']
    ).to(device)

    # 3. ìƒŒë‹ˆí‹° ì²´í¬ë¥¼ ìœ„í•´ ëª¨ë“  ë ˆì´ì–´ ì—´ê¸° (Unfreeze)
    # 3ì—í¬í¬ ê¸°ë‹¤ë¦¬ì§€ ë§ê³  ì§€ê¸ˆ ë°”ë¡œ ë‹¤ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ë§Œë“¤ì–´
    for param in model.parameters():
        param.requires_grad = True
    print("ğŸš€ All layers unfrozen for Sanity Check.")

    # 1. ì „ì²˜ë¦¬ ê·œì¹™ ì •ì˜ (Resize + Normalize)
    val_transform = A.Compose([
        A.Resize(224, 224), # ViT ê¸°ë³¸ ì…ë ¥ í¬ê¸°
        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ToTensorV2()
    ]) 
    
    # ë°ì´í„°ì…‹ ì¤€ë¹„ (ìƒ˜í”Œë§í•œ CSV ê²½ë¡œ ì‚¬ìš©)
    train_dataset = DeepfakeDataset(
        csv_path=tiny_csv_path, # ì„ì‹œ CSV ì‚¬ìš©
        img_dir=config['data']['img_dir'],
        processor=processor,
        num_frames=config['data']['num_frames'],
        transform=val_transform # ìƒŒë‹ˆí‹° ì²´í¬ëŠ” ì¦ê°• ì—†ì´ ê¹”ë”í•˜ê²Œ í…ŒìŠ¤íŠ¸!
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=4, # CPUë‹ˆê¹Œ ë°°ì¹˜ëŠ” ì‘ê²Œ
        shuffle=True
    )

    # ì˜µí‹°ë§ˆì´ì € (í•™ìŠµ ë°˜ì‘ì„ ë³´ê¸° ìœ„í•´ LRì„ ì¡°ê¸ˆ ë†’ê²Œ ì„¤ì •)
    optimizer = torch.optim.AdamW([
        {'params': model.model.vit.parameters(), 'lr': 1e-4}, 
        {'params': model.model.classifier.parameters(), 'lr': 1e-3}
    ], weight_decay=0.05)

    # âš ï¸ ë„¤ ì½”ë“œì— schedulerê°€ ì£¼ì„ì²˜ë¦¬ ë˜ì–´ìˆì–´ì„œ ì—ëŸ¬ ë‚  ìˆ˜ ìˆì–´!
    # í…ŒìŠ¤íŠ¸í•  ë•ŒëŠ” ì•„ë˜ í•œ ì¤„ì„ í™œì„±í™”í•˜ê±°ë‚˜, ë£¨í”„ ì•ˆì˜ scheduler.step()ì„ ì£¼ì„ì²˜ë¦¬í•´.
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)

    # í•™ìŠµ ë£¨í”„ (ìƒŒë‹ˆí‹° ì²´í¬ëŠ” 10~20 ì—í¬í¬ë§Œ ë´ë„ ì¶©ë¶„í•´)
    print("\n=== Start Sanity Check (100 Samples) ===")
    for epoch in range(20):
        train_loss = train_epoch(model, train_loader, torch.nn.CrossEntropyLoss(), optimizer, device)
        
        # 100ì¥ì— ëŒ€í•œ AUC ì§ì ‘ ê³„ì‚°í•´ì„œ ì¶œë ¥í•´ë³´ê¸°
        # (validate í•¨ìˆ˜ë¥¼ tiny_loaderì— ëŒ€í•´ ëŒë ¤ë„ ë¼)
        _, tiny_auc = validate(model, train_loader, torch.nn.CrossEntropyLoss(), device)
        
        print(f"Epoch {epoch+1} - Loss: {train_loss:.4f}, AUC: {tiny_auc:.4f}")
        
        if tiny_auc > 0.95:
            print("ğŸ‰ Success! ëª¨ë¸ì´ 100ì¥ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ê¸° ì‹œì‘í–ˆì–´.")
            break

if __name__ == '__main__':
    main()
