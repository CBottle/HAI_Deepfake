"""
HAI Deepfake Detection - Training Script

í•™ìŠµ ì½”ë“œ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
"""

import argparse
import os
import shutil
from pathlib import Path

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from transformers import ViTImageProcessor
from tqdm import tqdm
from sklearn.metrics import roc_auc_score
from torchvision import transforms
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np

from src.models import DeepfakeDetector
from src.dataset import DeepfakeDataset
from src.utils import (
    set_seed,
    load_config,
    save_checkpoint,
    load_checkpoint,
    get_device,
    AverageMeter
)


def parse_args():
    """ëª…ë ¹í–‰ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='Train Deepfake Detection Model')
    parser.add_argument('--config', type=str, default='config/config.yaml',
                        help='Path to config file')
    parser.add_argument('--resume', type=str, default=None,
                        help='Path to checkpoint to resume from')
    parser.add_argument('--debug', action='store_true',
                        help='Debug mode (small dataset)')
    return parser.parse_args()

def train_epoch(model, dataloader, criterion, optimizer, device, scaler=None):
    """
    í•œ ì—í¬í¬ í•™ìŠµ

    Args:
        model: í•™ìŠµí•  ëª¨ë¸
        dataloader: ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        optimizer: ì˜µí‹°ë§ˆì´ì €
        device: ë””ë°”ì´ìŠ¤
        scaler: GradScaler (Mixed Precision)

    Returns:
        í‰ê·  ì†ì‹¤
    """
    model.train()
    loss_meter = AverageMeter()

    pbar = tqdm(dataloader, desc='Training')
    for batch in pbar:
        pixel_values = batch['pixel_values'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()

        # Mixed Precision Training
        if scaler is not None:
            with torch.cuda.amp.autocast():
                logits = model(pixel_values)
                loss = criterion(logits, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            logits = model(pixel_values)
            loss = criterion(logits, labels)
            loss.backward()
            optimizer.step()

        loss_meter.update(loss.item(), pixel_values.size(0))
        pbar.set_postfix({'loss': f'{loss_meter.avg:.4f}'})

    return loss_meter.avg


def validate(model, dataloader, criterion, device):
    """
    ê²€ì¦

    Args:
        model: ëª¨ë¸
        dataloader: ë°ì´í„° ë¡œë”
        criterion: ì†ì‹¤ í•¨ìˆ˜
        device: ë””ë°”ì´ìŠ¤

    Returns:
        í‰ê·  ì†ì‹¤, ROC-AUC
    """
    model.eval()
    loss_meter = AverageMeter()

    all_probs = []
    all_labels = []

    with torch.no_grad():
        pbar = tqdm(dataloader, desc='Validation')
        for batch in pbar:
            pixel_values = batch['pixel_values'].to(device)
            labels = batch['labels'].to(device)

            logits = model(pixel_values)
            loss = criterion(logits, labels)

            probs = torch.softmax(logits, dim=1)[:, 1]  # Fake í™•ë¥ 

            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

            loss_meter.update(loss.item(), pixel_values.size(0))
            pbar.set_postfix({'loss': f'{loss_meter.avg:.4f}'})

    # ROC-AUC ê³„ì‚°
    auc = roc_auc_score(all_labels, all_probs)

    return loss_meter.avg, auc


# DFDC ì–¼êµ´ í¬ë¡­ ë°ì´í„°ì…‹ì— ì í•©í•œ ê°•ë ¥í•œ ì¦ê°• ì„¤ì •
hard_transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    # ì••ì¶• ì†ì‹¤: ë”¥í˜ì´í¬ íƒì§€ ëª¨ë¸ì´ ì €í™”ì§ˆ/ì••ì¶•ëœ í™˜ê²½ì—ì„œë„ ì˜ ì‘ë™í•˜ê²Œ í•¨
    A.ImageCompression(quality_lower=60, quality_upper=100, p=0.5),
    # ë¸”ëŸ¬/ë…¸ì´ì¦ˆ: ë‹¤ì–‘í•œ ìº¡ì²˜ í™˜ê²½ ì‹œë®¬ë ˆì´ì…˜
    A.OneOf([
        A.GaussianBlur(blur_limit=(3, 7)),
        A.GaussNoise(var_limit=(10.0, 50.0)),
    ], p=0.3),
    # ë°ê¸°/ëŒ€ë¹„ ë° ê¸°í•˜í•™ì  ë³€í™˜
    A.RandomBrightnessContrast(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=15, p=0.3),
])


def main():
    """3ë§Œ ì¥ ìƒ˜í”Œë§ ë° GPU í•™ìŠµ ë²„ì „"""
    args = parse_args()
    config = load_config(args.config)
    set_seed(config['experiment']['seed'])

    # 1. ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = get_device() 
    print(f"ğŸš€ í•™ìŠµ ì‹œì‘! ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    # ëª¨ë¸ ì´ˆê¸°í™”
    processor = ViTImageProcessor.from_pretrained(config['model']['name'])
    model = DeepfakeDetector(
        model_name=config['model']['name'],
        num_classes=config['model']['num_classes'],
        pretrained=config['model']['pretrained']
    ).to(device)

    # 2. ë°ì´í„° ìƒ˜í”Œë§ (3ë§Œ ì¥) - ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ ë°¸ëŸ°ì‹± ë¯¸ìˆ˜í–‰
    import pandas as pd
    train_csv_path = config['data']['train_csv']
    if os.path.exists(train_csv_path):
        full_df = pd.read_csv(train_csv_path)
        target_samples = 30000
        
        if len(full_df) > target_samples:
            train_df = full_df.sample(n=target_samples, random_state=42).reset_index(drop=True)
            print(f"ğŸ“Š {len(full_df)}ì¥ ì¤‘ {target_samples}ì¥ ëœë¤ ìƒ˜í”Œë§ ì™„ë£Œ (ë°¸ëŸ°ì‹± ë¯¸ìˆ˜í–‰)")
        else:
            train_df = full_df
            print(f"ğŸ“Š ì „ì²´ ë°ì´í„°({len(full_df)}ì¥)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # Dataset í´ë˜ìŠ¤ê°€ csv_pathë§Œ ë°›ìœ¼ë¯€ë¡œ ì„ì‹œ íŒŒì¼ ì €ì¥
        temp_train_csv = "temp_train_sampled.csv"
        train_df.to_csv(temp_train_csv, index=False)
        current_train_csv = temp_train_csv
    else:
        raise FileNotFoundError(f"âš ï¸ CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {train_csv_path}")

    # 3. ë°ì´í„°ì…‹ ë° ë¡œë”
    train_dataset = DeepfakeDataset(
        csv_path=current_train_csv,
        img_dir=config['data']['img_dir'],
        processor=processor,
        num_frames=config['data']['num_frames'],
        transform=hard_transform
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=config['training']['batch_size'],
        shuffle=True,
        num_workers=config['training']['num_workers'],
        pin_memory=True if device == 'cuda' else False
    )

    # 4. ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ (3ë§Œ ì¥ì— ë§ê²Œ T_max ì¡°ì ˆ)
    optimizer = optim.AdamW([
        {'params': model.model.vit.parameters(), 'lr': 1e-5},
        {'params': model.model.classifier.parameters(), 'lr': 5e-4}
    ], weight_decay=0.05)
    
    # 3ë§Œ ì¥ì´ë©´ 1ì—í­ì— ìŠ¤í…ì´ ë§ì§€ ì•Šìœ¼ë‹ˆ T_maxë¥¼ ì—í­ ìˆ˜ì— ë§ì¶°
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['training']['epochs'])


    # í•™ìŠµ ë£¨í”„ (ìƒŒë‹ˆí‹° ì²´í¬ëŠ” 10~20 ì—í¬í¬ë§Œ ë´ë„ ì¶©ë¶„í•´)
    print("\n=== Start Sanity Check (100 Samples) ===")
    for epoch in range(20):
        train_loss = train_epoch(model, train_loader, torch.nn.CrossEntropyLoss(), optimizer, device)
        
        # 100ì¥ì— ëŒ€í•œ AUC ì§ì ‘ ê³„ì‚°í•´ì„œ ì¶œë ¥í•´ë³´ê¸°
        # (validate í•¨ìˆ˜ë¥¼ tiny_loaderì— ëŒ€í•´ ëŒë ¤ë„ ë¼)
        _, tiny_auc = validate(model, train_loader, torch.nn.CrossEntropyLoss(), device)
        
        print(f"Epoch {epoch+1} - Loss: {train_loss:.4f}, AUC: {tiny_auc:.4f}")
        
        if tiny_auc > 0.95:
            print("ğŸ‰ Success! ëª¨ë¸ì´ 100ì¥ì˜ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ê¸° ì‹œì‘í–ˆì–´.")
            break

if __name__ == '__main__':
    main()
