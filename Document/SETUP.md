# HAI Deepfake - ê°œë°œí™˜ê²½ ì„¤ì • ê°€ì´ë“œ

> ë”¥í˜ì´í¬ íƒì§€ AI ëª¨ë¸ ê°œë°œì„ ìœ„í•œ ê°œë°œí™˜ê²½ ì„¤ì • ë¬¸ì„œ

## ğŸ“‹ ëª©ì°¨

1. [í™˜ê²½ ìš”êµ¬ì‚¬í•­](#í™˜ê²½-ìš”êµ¬ì‚¬í•­)
2. [í”„ë¡œì íŠ¸ êµ¬ì¡°](#í”„ë¡œì íŠ¸-êµ¬ì¡°)
3. [ë¡œì»¬ ê°œë°œ í™˜ê²½ ì„¤ì •](#ë¡œì»¬-ê°œë°œ-í™˜ê²½-ì„¤ì •)
4. [Colab ì‹¤í–‰ í™˜ê²½](#colab-ì‹¤í–‰-í™˜ê²½)
5. [ë°ì´í„° ê´€ë¦¬](#ë°ì´í„°-ê´€ë¦¬)
6. [Docker ì„¤ì •](#docker-ì„¤ì •)
7. [ì‹¤í—˜ ì¶”ì ](#ì‹¤í—˜-ì¶”ì )
8. [ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬](#ëª¨ë¸-ì²´í¬í¬ì¸íŠ¸-ê´€ë¦¬)
9. [Git ë¸Œëœì¹˜ ì „ëµ](#git-ë¸Œëœì¹˜-ì „ëµ)
10. [ê°œë°œ ì›Œí¬í”Œë¡œìš°](#ê°œë°œ-ì›Œí¬í”Œë¡œìš°)
11. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

---

## í™˜ê²½ ìš”êµ¬ì‚¬í•­

### Python í™˜ê²½
- **Python**: 3.10 ì´ìƒ (3.10 ê¶Œì¥)
- **íŒ¨í‚¤ì§€ ê´€ë¦¬ì**: Conda (ê¶Œì¥) ë˜ëŠ” venv

### GPU/CUDA (Colab/ì œì¶œ í™˜ê²½)
- **CUDA**: 11.8 ~ 12.6
- **PyTorch**: 2.5.0 ê¶Œì¥
- **ì¶”ë¡  í™˜ê²½**: L40S GPU (48GB VRAM)
- **í•™ìŠµ í™˜ê²½**: 48GB VRAM ë‚´ì—ì„œ ì‘ë™ í•„ìˆ˜

### ë¡œì»¬ ê°œë°œ ë„êµ¬
- **Git**: ë²„ì „ ê´€ë¦¬
- **VS Code**: ì½”ë“œ ì—ë””í„° (ê¶Œì¥)
- **Claude Code CLI**: AI ì½”ë”© ì–´ì‹œìŠ¤í„´íŠ¸

### OS ì§€ì›
- Windows 10/11
- macOS
- Linux (Ubuntu 20.04+)

---

## í”„ë¡œì íŠ¸ êµ¬ì¡°

### ì „ì²´ ë””ë ‰í† ë¦¬ êµ¬ì¡° (ëŒ€íšŒ ì œì¶œ ê¸°ì¤€)

```
HAI_Deepfake/
â”œâ”€â”€ model/                       # í•„ìˆ˜: ìµœì¢… ëª¨ë¸ ê°€ì¤‘ì¹˜
â”‚   â””â”€â”€ model.pt                 # ë‹¨ì¼ ëª¨ë¸ weight
â”‚
â”œâ”€â”€ src/                         # í¬ë§: ëª¨ë“ˆ ë¶„ë¦¬
â”‚   â”œâ”€â”€ models.py                # ëª¨ë¸ ì •ì˜
â”‚   â”œâ”€â”€ dataset.py               # ë°ì´í„° ë¡œë”/ì „ì²˜ë¦¬
â”‚   â””â”€â”€ utils.py                 # ê³µí†µ ìœ í‹¸ í•¨ìˆ˜
â”‚
â”œâ”€â”€ config/                      # í•„ìˆ˜: ì„¤ì • íŒŒì¼
â”‚   â””â”€â”€ config.yaml              # í•˜ì´í¼íŒŒë¼ë¯¸í„°, ê²½ë¡œ ë“±
â”‚
â”œâ”€â”€ env/                         # í•„ìˆ˜: í™˜ê²½ ì„¤ì •
â”‚   â”œâ”€â”€ Dockerfile               # Docker ì´ë¯¸ì§€ ì¬í˜„ìš©
â”‚   â”œâ”€â”€ requirements.txt         # Python ë¼ì´ë¸ŒëŸ¬ë¦¬ ëª©ë¡
â”‚   â””â”€â”€ environment.yml          # Conda í™˜ê²½ ì •ì˜
â”‚
â”œâ”€â”€ train_data/                  # í•„ìˆ˜: í•™ìŠµ ë°ì´í„°
â”‚   â””â”€â”€ [í•™ìŠµ ë°ì´í„° + ì¶œì²˜/ë¼ì´ì„ ìŠ¤ ì •ë³´]
â”‚
â”œâ”€â”€ test_data/                   # í•„ìˆ˜: í‰ê°€ ë°ì´í„°
â”‚   â””â”€â”€ [ëŒ€íšŒ ì œê³µ í‰ê°€ ë°ì´í„°ì…‹]
â”‚
â”œâ”€â”€ notebooks/                   # ê°œë°œìš©: Jupyter ë…¸íŠ¸ë¶
â”‚   â”œâ”€â”€ train.ipynb              # Colab í•™ìŠµìš©
â”‚   â”œâ”€â”€ inference.ipynb          # Colab ì¶”ë¡ ìš©
â”‚   â””â”€â”€ eda.ipynb                # ë°ì´í„° ë¶„ì„ìš©
â”‚
â”œâ”€â”€ checkpoints/                 # í•™ìŠµ ì¤‘ ì²´í¬í¬ì¸íŠ¸
â”‚   â””â”€â”€ [epochë³„ ëª¨ë¸ ì €ì¥]
â”‚
â”œâ”€â”€ submissions/                 # ì œì¶œ íŒŒì¼
â”‚   â””â”€â”€ submission_*.csv
â”‚
â”œâ”€â”€ train.py                     # í•„ìˆ˜: í•™ìŠµ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
â”œâ”€â”€ inference.py                 # í•„ìˆ˜: ì¶”ë¡  ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
â”œâ”€â”€ eval.py                      # í¬ë§: ê²€ì¦ìš© í‰ê°€ ì½”ë“œ
â”‚
â”œâ”€â”€ README.md                    # í•„ìˆ˜: í”„ë¡œì íŠ¸ ì„¤ëª…
â”œâ”€â”€ SETUP.md                     # ê°œë°œí™˜ê²½ ì„¤ì • ê°€ì´ë“œ (ë³¸ ë¬¸ì„œ)
â”œâ”€â”€ Rule.md                      # ëŒ€íšŒ ê·œì¹™
â”œâ”€â”€ baseline.ipynb               # Baseline ì½”ë“œ
â””â”€â”€ .gitignore                   # Git ì œì™¸ íŒŒì¼
```

### íŒŒì¼ í˜•ì‹ ì „ëµ

**ê°œë°œ/ì‹¤í—˜ ë‹¨ê³„**: `.ipynb` (Jupyter Notebook)
- Colabì—ì„œ ë°”ë¡œ ì‹¤í–‰
- ì‹œê°í™” ë° ì¸í„°ë™í‹°ë¸Œ ê°œë°œ
- ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘

**ìµœì¢… ì œì¶œìš©**: `.py` (Python ìŠ¤í¬ë¦½íŠ¸)
- Docker í™˜ê²½ì—ì„œ ì‹¤í–‰
- ëŒ€íšŒ ê·œì¹™ ì¤€ìˆ˜
- ì¬í˜„ì„± ë³´ì¥

---

## ë¡œì»¬ ê°œë°œ í™˜ê²½ ì„¤ì •

### 1. Git ì €ì¥ì†Œ í´ë¡ 

```bash
# GitHub ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/CBottle/HAI_Deepfake.git
cd HAI_Deepfake
```

### 2. Conda ê°€ìƒí™˜ê²½ ìƒì„±

```bash
# Conda í™˜ê²½ ìƒì„± (Python 3.10)
conda create -n deepfake python=3.10 -y
conda activate deepfake
```

### 3. ì˜ì¡´ì„± ì„¤ì¹˜

```bash
# PyTorch ì„¤ì¹˜ (CUDA 12.1 ê¸°ì¤€, ë¡œì»¬ í™˜ê²½ì— ë§ê²Œ ì¡°ì •)
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia -y

# ê¸°íƒ€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
pip install -r env/requirements.txt
```

**requirements.txt ì˜ˆì‹œ**:
```txt
numpy>=1.24.0
pandas>=2.0.0
opencv-python>=4.8.0
Pillow>=10.0.0
transformers>=4.35.0
scikit-learn>=1.3.0
tqdm>=4.65.0
PyYAML>=6.0
wandb>=0.16.0
```

### 4. ë¡œì»¬ í…ŒìŠ¤íŠ¸ (CPU)

```bash
# ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì½”ë“œ í…ŒìŠ¤íŠ¸
python train.py --debug --epochs 1 --batch_size 2 --device cpu
```

---

## Colab ì‹¤í–‰ í™˜ê²½

### 1. GitHub ì €ì¥ì†Œ ì—°ë™

**Colab ë…¸íŠ¸ë¶ ì²« ì…€**:
```python
# GitHub ì €ì¥ì†Œ í´ë¡ 
!git clone https://github.com/CBottle/HAI_Deepfake.git
%cd HAI_Deepfake

# ìµœì‹  ì½”ë“œ ì—…ë°ì´íŠ¸ (ì´ë¯¸ í´ë¡ í•œ ê²½ìš°)
!git pull origin main
```

### 2. Google Drive ë§ˆìš´íŠ¸ (ë°ì´í„°ìš©)

```python
from google.colab import drive
drive.mount('/content/drive')

# ë°ì´í„° ê²½ë¡œ ì‹¬ë³¼ë¦­ ë§í¬
!ln -s /content/drive/MyDrive/HAI_Deepfake/train_data ./train_data
!ln -s /content/drive/MyDrive/HAI_Deepfake/test_data ./test_data
```

### 3. í™˜ê²½ ì„¤ì •

```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
!pip install -r env/requirements.txt

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ê²½ë¡œ ì¶”ê°€
import sys
sys.path.append('/content/HAI_Deepfake/src')
```

### 4. í•™ìŠµ ì‹¤í–‰

```python
# train.ipynbì—ì„œ ì‹¤í–‰
!python train.py --config config/config.yaml --device cuda
```

### 5. ì¶”ë¡  ì‹¤í–‰

```python
# inference.ipynbì—ì„œ ì‹¤í–‰
!python inference.py --model model/model.pt --test_dir test_data --output submissions/
```

### 6. ê²°ê³¼ ì €ì¥ (Driveë¡œ)

```python
# í•™ìŠµëœ ëª¨ë¸ì„ Driveì— ì €ì¥
!cp model/model.pt /content/drive/MyDrive/HAI_Deepfake/model/
!cp -r checkpoints /content/drive/MyDrive/HAI_Deepfake/
```

---

## ë°ì´í„° ê´€ë¦¬

### ë°ì´í„° ì €ì¥ ìœ„ì¹˜

**1. Google Drive** (ê¶Œì¥)
```
Google Drive/
â””â”€â”€ HAI_Deepfake/
    â”œâ”€â”€ train_data/           # í•™ìŠµ ë°ì´í„° (10GB ~ 100GB+)
    â”‚   â”œâ”€â”€ real/
    â”‚   â””â”€â”€ fake/
    â””â”€â”€ test_data/            # ëŒ€íšŒ ì œê³µ í‰ê°€ ë°ì´í„°
```

**ì¥ì **:
- Colabê³¼ ì—°ë™ ì‰¬ì›€
- 15GB ë¬´ë£Œ (ìœ ë£Œ í™•ì¥ ê°€ëŠ¥)

**ë‹¨ì **:
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì—…ë¡œë“œ ì‹œê°„ ì†Œìš”

---

**2. Kaggle Datasets** (ê³µê°œ ë°ì´í„°)
```python
# Colabì—ì„œ Kaggle API ì‚¬ìš©
!pip install kaggle

# Kaggle API í† í° ì—…ë¡œë“œ (~/.kaggle/kaggle.json)
!mkdir -p ~/.kaggle
!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
!kaggle datasets download -d [dataset-name]
!unzip [dataset-name].zip -d train_data/
```

**ì¥ì **:
- ë¹ ë¥¸ ë‹¤ìš´ë¡œë“œ
- ê³µê°œ ë°ì´í„°ì…‹ í™œìš© ìš©ì´

---

### ë°ì´í„°ì…‹ ì¶œì²˜ ë¬¸ì„œí™”

**train_data/README.md** ìƒì„± (ëŒ€íšŒ ì œì¶œ ì‹œ í•„ìˆ˜):
```markdown
# í•™ìŠµ ë°ì´í„° ì¶œì²˜

## ë°ì´í„°ì…‹ ëª©ë¡
1. FaceForensics++ (https://github.com/ondyari/FaceForensics)
   - ë¼ì´ì„ ìŠ¤: [ë¼ì´ì„ ìŠ¤ ì •ë³´]
   - ì‚¬ìš©ëŸ‰: 10,000ì¥

2. Celeb-DF (http://www.cs.albany.edu/~lsw/celeb-deepfakeforensics.html)
   - ë¼ì´ì„ ìŠ¤: [ë¼ì´ì„ ìŠ¤ ì •ë³´]
   - ì‚¬ìš©ëŸ‰: 5,000ì¥

## ë°ì´í„° ì „ì²˜ë¦¬
- í”„ë ˆì„ ì¶”ì¶œ: 10 fps
- ì´ë¯¸ì§€ í¬ê¸°: 224x224
- ì¦ê°•: RandomHorizontalFlip, ColorJitter
```

---

## Docker ì„¤ì •

### Dockerfile ì‘ì„± (env/Dockerfile)

```dockerfile
# Base image (ëŒ€íšŒ í™˜ê²½ ê¸°ì¤€)
FROM pytorch/pytorch:2.5.0-cuda12.1-cudnn9-runtime

# ì‘ì—… ë””ë ‰í† ë¦¬
WORKDIR /workspace

# ì‹œìŠ¤í…œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
RUN apt-get update && apt-get install -y \
    libgl1-mesa-glx \
    libglib2.0-0 \
    && rm -rf /var/lib/apt/lists/*

# Python ì˜ì¡´ì„± ì„¤ì¹˜
COPY env/requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# í”„ë¡œì íŠ¸ íŒŒì¼ ë³µì‚¬
COPY . .

# ê¸°ë³¸ ëª…ë ¹ì–´
CMD ["python", "inference.py"]
```

### Docker ì´ë¯¸ì§€ ë¹Œë“œ ë° í…ŒìŠ¤íŠ¸

```bash
# Docker ì´ë¯¸ì§€ ë¹Œë“œ
docker build -t hai-deepfake:latest -f env/Dockerfile .

# ë¡œì»¬ì—ì„œ í…ŒìŠ¤íŠ¸
docker run --gpus all -v $(pwd)/test_data:/workspace/test_data \
    hai-deepfake:latest python inference.py --test_dir test_data
```

### requirements.txt ìµœì¢… í™•ì¸

```bash
# í˜„ì¬ í™˜ê²½ì˜ íŒ¨í‚¤ì§€ ëª©ë¡ ì €ì¥
pip freeze > env/requirements.txt

# ë¶ˆí•„ìš”í•œ íŒ¨í‚¤ì§€ ì œê±° (ìˆ˜ë™ í¸ì§‘)
# Docker ì´ë¯¸ì§€ í¬ê¸° ìµœì í™”
```

---

## ì‹¤í—˜ ì¶”ì 

### Weights & Biases (wandb) ì„¤ì •

**1. ì„¤ì¹˜ ë° ë¡œê·¸ì¸**:
```bash
pip install wandb
wandb login
```

**2. í•™ìŠµ ì½”ë“œì— í†µí•©**:
```python
import wandb

# ì‹¤í—˜ ì‹œì‘
wandb.init(
    project="hai-deepfake",
    name="vit-base-exp1",
    config={
        "learning_rate": 1e-4,
        "batch_size": 32,
        "epochs": 50,
        "model": "ViT-Base"
    }
)

# í•™ìŠµ ì¤‘ ë¡œê¹…
for epoch in range(epochs):
    train_loss = train_epoch(...)
    val_auc = validate(...)

    wandb.log({
        "epoch": epoch,
        "train_loss": train_loss,
        "val_auc": val_auc
    })

# ì‹¤í—˜ ì¢…ë£Œ
wandb.finish()
```

**3. ì‹¤í—˜ ë¹„êµ**:
- wandb.aiì—ì„œ ì—¬ëŸ¬ ì‹¤í—˜ ê²°ê³¼ ì‹œê°í™”
- í•˜ì´í¼íŒŒë¼ë¯¸í„°ë³„ ì„±ëŠ¥ ë¹„êµ

---

### TensorBoard (ëŒ€ì•ˆ)

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/exp1')

for epoch in range(epochs):
    writer.add_scalar('Loss/train', train_loss, epoch)
    writer.add_scalar('AUC/val', val_auc, epoch)

writer.close()
```

```bash
# TensorBoard ì‹¤í–‰
tensorboard --logdir=runs
```

---

## ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬

### ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì „ëµ

```python
import torch
import os

def save_checkpoint(model, optimizer, epoch, val_auc, checkpoint_dir='checkpoints'):
    os.makedirs(checkpoint_dir, exist_ok=True)

    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'val_auc': val_auc
    }

    # Epochë³„ ì²´í¬í¬ì¸íŠ¸
    checkpoint_path = f'{checkpoint_dir}/checkpoint_epoch_{epoch:03d}.pt'
    torch.save(checkpoint, checkpoint_path)

    # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥
    best_path = f'{checkpoint_dir}/best_model.pt'
    if not os.path.exists(best_path) or val_auc > get_best_auc(best_path):
        torch.save(checkpoint, best_path)
        print(f'Best model updated! AUC: {val_auc:.4f}')

def load_checkpoint(model, optimizer, checkpoint_path):
    checkpoint = torch.load(checkpoint_path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    return checkpoint['epoch'], checkpoint['val_auc']
```

### ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
checkpoints/
â”œâ”€â”€ checkpoint_epoch_001.pt
â”œâ”€â”€ checkpoint_epoch_002.pt
â”œâ”€â”€ ...
â”œâ”€â”€ checkpoint_epoch_050.pt
â””â”€â”€ best_model.pt              # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
```

### ë””ìŠ¤í¬ ê³µê°„ ê´€ë¦¬

```python
# ìµœê·¼ Nê°œ ì²´í¬í¬ì¸íŠ¸ë§Œ ìœ ì§€
def cleanup_checkpoints(checkpoint_dir, keep_last=5):
    checkpoints = sorted(glob.glob(f'{checkpoint_dir}/checkpoint_epoch_*.pt'))
    if len(checkpoints) > keep_last:
        for old_ckpt in checkpoints[:-keep_last]:
            os.remove(old_ckpt)
            print(f'Removed old checkpoint: {old_ckpt}')
```

---

## Git ë¸Œëœì¹˜ ì „ëµ

### ë¸Œëœì¹˜ êµ¬ì¡°

```
main                    # ì•ˆì • ë²„ì „ (ì œì¶œ ê°€ëŠ¥í•œ ì½”ë“œ)
  â”œâ”€â”€ develop           # ê°œë°œ í†µí•© ë¸Œëœì¹˜
  â”‚   â”œâ”€â”€ feature/vit-large        # ìƒˆ ëª¨ë¸ ì‹¤í—˜
  â”‚   â”œâ”€â”€ feature/data-augment     # ë°ì´í„° ì¦ê°• ì‹¤í—˜
  â”‚   â””â”€â”€ feature/ensemble         # ì•™ìƒë¸” ì‹¤í—˜ (ê·œì¹™ ìœ„ë°˜ ì‹œ ì‚­ì œ)
  â””â”€â”€ hotfix/inference-bug         # ê¸´ê¸‰ ë²„ê·¸ ìˆ˜ì •
```

### ë¸Œëœì¹˜ ì‚¬ìš© ê·œì¹™

**1. main ë¸Œëœì¹˜**:
- í•­ìƒ ì‹¤í–‰ ê°€ëŠ¥í•œ ìƒíƒœ ìœ ì§€
- ì œì¶œ ê°€ëŠ¥í•œ ì½”ë“œë§Œ merge
- Pull Request í•„ìˆ˜

**2. develop ë¸Œëœì¹˜**:
- ì‹¤í—˜ í†µí•©ìš©
- ë§¤ì¼ ì‘ì—… ë‚´ìš© merge

**3. feature ë¸Œëœì¹˜**:
- ìƒˆë¡œìš´ ì‹¤í—˜/ê¸°ëŠ¥ ê°œë°œ
- ë„¤ì´ë°: `feature/[ê¸°ëŠ¥ëª…]`

**4. hotfix ë¸Œëœì¹˜**:
- ê¸´ê¸‰ ë²„ê·¸ ìˆ˜ì •
- ë„¤ì´ë°: `hotfix/[ë²„ê·¸ëª…]`

---

### Git ì›Œí¬í”Œë¡œìš°

```bash
# 1. ìƒˆ ì‹¤í—˜ ì‹œì‘
git checkout develop
git pull origin develop
git checkout -b feature/vit-large

# 2. ì‘ì—… ë° ì»¤ë°‹
git add .
git commit -m "Add ViT-Large model implementation"

# 3. ì›ê²© ì €ì¥ì†Œì— í‘¸ì‹œ
git push origin feature/vit-large

# 4. Pull Request ìƒì„± (GitHub ì›¹)
# develop <- feature/vit-large

# 5. ì‹¤í—˜ ì„±ê³µ ì‹œ developì— merge
git checkout develop
git merge feature/vit-large

# 6. ê²€ì¦ ì™„ë£Œ í›„ mainì— merge
git checkout main
git merge develop
git push origin main
```

---

## ê°œë°œ ì›Œí¬í”Œë¡œìš°

### ì „ì²´ ê°œë°œ ì‚¬ì´í´

```
1. ë¡œì»¬ ê°œë°œ (Claude CLI ì‚¬ìš©)
   â”œâ”€â”€ src/ ëª¨ë“ˆ ì‘ì„±/ìˆ˜ì •
   â”œâ”€â”€ notebooks/ ì‹¤í—˜ ì½”ë“œ ì‘ì„±
   â””â”€â”€ config/ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
           â†“
2. Git ì»¤ë°‹ ë° í‘¸ì‹œ
   â”œâ”€â”€ git add .
   â”œâ”€â”€ git commit -m "ë©”ì‹œì§€"
   â””â”€â”€ git push origin [ë¸Œëœì¹˜]
           â†“
3. Colabì—ì„œ ì‹¤í–‰
   â”œâ”€â”€ git pull
   â”œâ”€â”€ í•™ìŠµ ì‹¤í–‰ (train.ipynb)
   â”œâ”€â”€ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (Drive)
   â””â”€â”€ wandbë¡œ ê²°ê³¼ ì¶”ì 
           â†“
4. ê²°ê³¼ ë¶„ì„ ë° ê°œì„ 
   â”œâ”€â”€ wandbì—ì„œ ì‹¤í—˜ ë¹„êµ
   â”œâ”€â”€ ë² ìŠ¤íŠ¸ ëª¨ë¸ ì„ ì •
   â””â”€â”€ ë‹¤ìŒ ì‹¤í—˜ ê³„íš
           â†“
5. ìµœì¢… ì œì¶œ ì¤€ë¹„
   â”œâ”€â”€ .ipynb â†’ .py ë³€í™˜
   â”œâ”€â”€ Docker í…ŒìŠ¤íŠ¸
   â””â”€â”€ ì œì¶œ íŒŒì¼ ìƒì„±
```

---

### ì¼ì¼ ì‘ì—… ë£¨í‹´

**ì•„ì¹¨**:
```bash
# 1. ìµœì‹  ì½”ë“œ ë™ê¸°í™”
git pull origin develop

# 2. ìƒˆ ì‹¤í—˜ ë¸Œëœì¹˜ ìƒì„±
git checkout -b feature/new-experiment
```

**ì‘ì—… ì¤‘**:
```bash
# 3. ë¡œì»¬ì—ì„œ ì½”ë“œ ì‘ì„± (Claude CLI)
# 4. ì‘ì€ í…ŒìŠ¤íŠ¸ (CPU)
python train.py --debug

# 5. ì»¤ë°‹ (ìì£¼)
git add .
git commit -m "WIP: Add new augmentation"
git push origin feature/new-experiment
```

**ì €ë…**:
```bash
# 6. Colabì—ì„œ ë³¸ê²© í•™ìŠµ
# 7. ê²°ê³¼ í™•ì¸ í›„ developì— merge
git checkout develop
git merge feature/new-experiment
git push origin develop
```

---

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. Colab ì„¸ì…˜ ëŠê¹€

**ë¬¸ì œ**: í•™ìŠµ ì¤‘ ì„¸ì…˜ ì¢…ë£Œ
**í•´ê²°**:
- ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ:
  ```python
  if os.path.exists('checkpoints/best_model.pt'):
      start_epoch, _ = load_checkpoint(model, optimizer, 'checkpoints/best_model.pt')
  else:
      start_epoch = 0
  ```
- Colab Pro ê³ ë ¤

---

### 2. CUDA Out of Memory

**ë¬¸ì œ**: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±
**í•´ê²°**:
- ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê°ì†Œ:
  ```yaml
  batch_size: 16  # 32 â†’ 16
  ```
- Gradient Accumulation:
  ```python
  accumulation_steps = 4
  for i, batch in enumerate(dataloader):
      loss = model(batch) / accumulation_steps
      loss.backward()
      if (i + 1) % accumulation_steps == 0:
          optimizer.step()
          optimizer.zero_grad()
  ```
- Mixed Precision Training:
  ```python
  from torch.cuda.amp import autocast, GradScaler
  scaler = GradScaler()

  with autocast():
      loss = model(batch)
  scaler.scale(loss).backward()
  scaler.step(optimizer)
  scaler.update()
  ```

---

### 3. Git ì¶©ëŒ

**ë¬¸ì œ**: merge ì‹œ ì¶©ëŒ ë°œìƒ
**í•´ê²°**:
```bash
# ì¶©ëŒ íŒŒì¼ í™•ì¸
git status

# ì¶©ëŒ í•´ê²° í›„
git add [í•´ê²°ëœ íŒŒì¼]
git commit -m "Resolve merge conflict"
```

---

### 4. ëª¨ë“ˆ Import ì˜¤ë¥˜ (Colab)

**ë¬¸ì œ**: `ModuleNotFoundError: No module named 'src'`
**í•´ê²°**:
```python
import sys
sys.path.append('/content/HAI_Deepfake')

# ë˜ëŠ” ìƒëŒ€ ê²½ë¡œ ì‚¬ìš©
from src.models import MyModel
```

---

### 5. Docker ë¹Œë“œ ì‹¤íŒ¨

**ë¬¸ì œ**: Docker ì´ë¯¸ì§€ ë¹Œë“œ ì¤‘ ì˜¤ë¥˜
**í•´ê²°**:
```bash
# ìºì‹œ ì—†ì´ ë¹Œë“œ
docker build --no-cache -t hai-deepfake:latest -f env/Dockerfile .

# ë¹Œë“œ ë¡œê·¸ ìì„¸íˆ ë³´ê¸°
docker build --progress=plain -t hai-deepfake:latest -f env/Dockerfile .
```

---

### 6. ì¶”ë¡  ì‹œê°„ ì´ˆê³¼ (60ë¶„ ì œí•œ)

**ë¬¸ì œ**: ì¶”ë¡ ì´ 60ë¶„ì„ ì´ˆê³¼
**í•´ê²°**:
- ë°°ì¹˜ ì¶”ë¡  ìµœì í™”:
  ```python
  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
  batch_size = 64  # ë©”ëª¨ë¦¬ í—ˆìš© ë²”ìœ„ ë‚´

  # DataLoader num_workers ì¦ê°€
  dataloader = DataLoader(dataset, batch_size=64, num_workers=4)
  ```
- TorchScript ì‚¬ìš©:
  ```python
  model = torch.jit.script(model)
  ```
- FP16 ì¶”ë¡ :
  ```python
  model.half()
  inputs = inputs.half()
  ```

---

### 7. wandb ë¡œê·¸ì¸ ì˜¤ë¥˜ (Colab)

**ë¬¸ì œ**: Colabì—ì„œ wandb ë¡œê·¸ì¸ ì•ˆ ë¨
**í•´ê²°**:
```python
# API í‚¤ ì§ì ‘ ì…ë ¥
import wandb
wandb.login(key='your-api-key')

# ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ ì„¤ì •
import os
os.environ['WANDB_API_KEY'] = 'your-api-key'
```

---

## ì°¸ê³  ìë£Œ

- **ëŒ€íšŒ ê·œì¹™**: [Rule.md](Rule.md)
- **Baseline ì½”ë“œ**: [baseline.ipynb](baseline.ipynb)
- **PyTorch ë¬¸ì„œ**: https://pytorch.org/docs/stable/index.html
- **Transformers ë¬¸ì„œ**: https://huggingface.co/docs/transformers
- **wandb ë¬¸ì„œ**: https://docs.wandb.ai/
- **Docker ë¬¸ì„œ**: https://docs.docker.com/

---

## ë²„ì „ ì •ë³´

- **ë¬¸ì„œ ë²„ì „**: 1.0.0
- **ìµœì¢… ìˆ˜ì •ì¼**: 2026-01-02
- **ì‘ì„±ì**: HAI Deepfake Team
